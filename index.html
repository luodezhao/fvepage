<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="FVE">
  <meta property="og:title" content="Generative Video Diffusion for Unseen Cross-Domain Video Moment Retrieval"/>
<!--   <meta property="og:description" content="Generative Video Diffusion for Unseen Cross-Domain Video Moment Retrieval"/> -->
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
<!--   <meta property="og:image" content="GenSAM_logo.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->
  
  <meta name="twitter:title" content="Generative Video Diffusion for Unseen Cross-Domain Video Moment Retrieval">
<!--   <meta name="twitter:description" content="Generative Video Diffusion for Unseen Cross-Domain Video Moment Retrieval"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
<!--   <meta name="twitter:image" content="GenSAM_logo.png"> -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>FVE</title>
<!--   <link rel="icon" type="image/x-icon" href="GenSAM_logo.png"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Generative Video Diffusion for Unseen Cross-Domain Video Moment Retrieval</h1>
          <div class="is-size-5 publication-authors">
  <span class="author-block">
    <a href="https://luodezhao.github.io/" target="_blank">Dezhao Luo</a><sup>1</sup>,
  </span>
  <span class="author-block">
    <a href="http://www.eecs.qmul.ac.uk/~sgg/" target="_blank">Shaogang Gong</a><sup>1</sup>,
  </span>
  <span class="author-block">
    <a href="https://raymond-sci.github.io/" target="_blank">Jiabo Huang</a><sup>2</sup>,
  </span>
  <span class="author-block">
    <a href="https://sites.google.com/view/hailinjin" target="_blank">Hailin Jin</a><sup>3</sup>,
  </span>
  <span class="author-block">
    <a href="http://www.csyangliu.com/" target="_blank">Yang Liu</a><sup>4,5</sup>
  </span>
</div>

<div class="is-size-5 publication-affiliations">
  <span class="affiliation-block"><sup>1</sup> Queen Mary University of London</span><br>
  <span class="affiliation-block"><sup>2</sup> Sony AI</span><br>
  <span class="affiliation-block"><sup>3</sup> Adobe Research</span><br>
  <span class="affiliation-block"><sup>4</sup> WICT, Peking University</span><br>
  <span class="affiliation-block"><sup>5</sup> State Key Laboratory of General Artificial Intelligence, Peking University</span>
</div>


                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                     <!-- ArXiv abstract Link -->
                    <span class="link-block">
                    <a href="https://arxiv.org/pdf/2401.13329" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>paper</span>
                    </a>
                    </span>
                         <span class="link-block">
              <a href="https://arxiv.org/abs/2401.13329" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv </span>
              </a>
            </span>

                    <!-- Supplementary PDF link -->
<!--                     <span class="link-block">
                      <a href="supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
<!--                   <span class="link-block">
                    <a href="https://github.com/jyLin8100/InvSeg" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <style>
            .content {
              font-family: "Times New Roman", Times, serif;
            }
          </style>
          <p>
         Video moment retrieval (VMR) aims to locate the most likely video moment(s) corresponding to a text query in untrimmed videos. Training of existing methods is limited by
  the lack of diverse and generalisable VMR datasets, hindering
their ability to generalise moment-text associations to
queries containing novel semantic concepts (unseen both visually
  and textually in a training source domain). For model generalisation to novel semantics, existing methods rely heavily on assuming to have access to both video and text sentence pairs from a
  target domain in addition to the source domain pair-wise training data. This is neither practical nor scalable.
In this work, we introduce a more generalisable approach by
  assuming  only text sentences describing new semantics are available in model training without having seen any videos from a target domain. To that end, we propose a Fine-grained Video Editing framework, termed 
  FVE, that explores generative video
diffusion to facilitate fine-grained video editing from the seen
  source concepts to the unseen target sentences consisting of new
  concepts. This enables generative hypotheses of unseen video
  moments corresponding to the novel concepts in the target
  domain. This fine-grained generative video diffusion  
retains the original video structure and subject specifics from the
source domain while introducing semantic distinctions of unseen
novel vocabularies in the target domain. A critical challenge
  is how to enable this generative fine-grained diffusion process to
  be meaningful in optimising VMR, more than just synthesising
  visually pleasing videos. We solve this problem by introducing a
  hybrid selection mechanism that integrates three quantitative
  metrics to selectively incorporate synthetic video 
moments (novel video hypotheses) as enlarged additions to the
  original source training data, whilst minimising potential
detrimental noise or unnecessary repetitions in the novel synthetic
videos harmful to VMR learning. Experiments on three datasets
demonstrate the effectiveness of FVE to unseen novel semantic
video moment retrieval tasks.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

  <!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
        <h2 class="title is-2 has-text-centered">Our Framework</h2>
        <img src="method.jpg" alt="MY ALT TEXT" style="width: 1000px; height: auto; margin-top: 30px; display: block;">

        <h2 class="subtitle" style="max-width: 75%; margin: 0 auto; text-align: justify;">
          <br>Our designed instance-preserving action editing model. We first take the video as a set of images and train an image diffusion model to align a special text token with the instance shared between those frames. Subsequently, we take those frames as a sequence and freeze the layers in the image diffusion model, and append a temporal layer to capture the video motions.
    </div>
  </div>
</div>
<!-- End image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
        <h2 class="title is-2 has-text-centered">Hybrid Data Selection</h2>
        <img src="dataselection.png" alt="MY ALT TEXT" style="width: 1000px; height: auto; margin-top: 30px; display: block;">

        <h2 class="subtitle" style="max-width: 75%; margin: 0 auto; text-align: justify;">
          <br>Data generation and hybrid selection. For data
generation, we first train the video diffusion model to
align moment with a sentence, then we use an editing prompt to edit the moment. The hybrid selection strategy includes a cross-modal relevance and unimodal structure score to select high-quality generation, as
well as a model performance disparity to select beneficial
data for VMR training.
    </div>
  </div>
</div>
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
        <h2 class="title is-2 has-text-centered">Comparison with SOTA</h2>
        <img src="experiment.jpg" alt="MY ALT TEXT" style="width: 400px; height: auto; margin-top: 30px; display: block;">
        <h2 class="subtitle" style="max-width: 75%; margin: 0 auto; text-align: justify;">
          <br><strong>Comparison under VMR task</strong>
    </div>
  </div>
</div>
<!-- End image carousel -->

  
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
    <h2 class="title is-2 has-text-centered">Visualization</h2>
    <img src="visual.jpg" alt="MY ALT TEXT" style="width: 1000px; height: auto; margin-top: 30px; display: block;">
    <h2 class="subtitle" style="max-width: 75%; margin: 0 auto; text-align: justify;">
        Examples of Video Editing. 
    </h2>
   
</div>

  </div>
</div>
<!-- End image carousel -->
<!--   <iframe src="aaai25.pdf" width="100%" height="600px" style="margin-top: 30px; display: block;"></iframe> -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
      <h2 class="title is-2 has-text-centered">Poster</h2>
      <iframe src="aaai25.pdf" width="1000px" height="450px" style="margin-top: 30px; display: block; border: none;"></iframe>
    </div>
  </div>
</section>
  
<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{luo2024generative,
  title={Generative Video Diffusion for Unseen Cross-Domain Video Moment Retrieval},
  author={Luo, Dezhao and Gong, Shaogang and Huang, Jiabo and Jin, Hailin and Liu, Yang},
  journal={arXiv preprint arXiv:2401.13329},
  year={2024}
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
